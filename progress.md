# Project Progress & Status

**Last Updated:** December 12, 2025

## ðŸŸ¢ Current System Status
**Overall**: Fully Functional / Maintenance Mode
**Frontend**: Active (Port 3000)
**Backend**: Active (Port 8001)
**Database**: MongoDB (Port 27017)
**AI**: Hybrid (Ollama Primary -> Local Fallback -> Mock Backup)

---

## ðŸ›  Recent Fixes & Improvements Consolidations

### 1. Startup & Infrastructure (From `STARTUP_FIXES.md`, `STARTUP_FIX_SUMMARY.md`)
*   **Docker Networking**: Implemented `docker-compose` to place MongoDB and Ollama in the same `roddle_default` network.
*   **Batch Scripts**:
    *   `dev.bat`: Now opens separate terminal windows for Frontend and Backend with live logs.
    *   `start.bat`: Production-mode script. Runs in background, logs to files, and **auto-opens browser**.
    *   `stop.bat`: improved to use `docker-compose down` for clean shutdown.
*   **Syntax Fixes**: Resolved variable expansion and redirection errors in Windows batch scripts.

### 2. Ollama AI Integration (From `OLLAMA_DEBUG.md`, `RECOVERY_SUMMARY.md`)
*   **Fallback Architecture**: Created a robust chain:
    1.  **Ollama (Neural-Chat)**: Primary, creative generation.
    2.  **Local LLM**: Secondary fallback (HuggingFace transformers).
    3.  **Mock LLM**: Final safety net (Pre-defined riddles).
*   **Verification**: Added `backend/ollama_llm.py` and `setup-ollama.ps1` for easy installation.
*   **Health Checks**: `/api/health` shows detailed LLM status.

### 3. Logging & Debugging (From `UPDATE_SUMMARY.md`, `FINAL_FIXES_SUMMARY.md`)
*   **Dual Logging**: Output is split between Console (for dev) and Log Files (`backend.log`, `frontend.log`).
*   **Timestamping**: Fixed log format to include timestamps for better debugging.
*   **Startup Diagnostics**: Backend now prints a "LLM Status on Startup" banner to clearly indicate which AI mode is active.

### 4. Game Logic (From `FINAL_FIXES_SUMMARY.md`)
*   **Race Condition Fix**: Implemented `asyncio.Lock()` in `server.py` to prevent "10 riddles generated" bug where multiple requests triggered simultaneous generations.

---

## ðŸ“‹ Troubleshooting & Maintenance Guide

### Development Commands
| Action | Command | Description |
|ion | Command | Description |
|--------|---------|-------------|
| **Start Dev** | `.\dev.bat` | Opens terminals, starts all services |
| **Start Prod** | `.\start.bat` | Background mode, opens browser |
| **Stop All** | `.\stop.bat` | Stops containers and node processes |
| **Setup AI** | `.\setup-ollama.ps1` | Downloads Ollama & Model |

### Debugging Ollama
If backend says "Using Mock LLM":
1.  Check container: `docker ps | findstr ollama`
2.  Check model: `docker exec riddle-ollama ollama list`
3.  Test connectivity: `curl http://localhost:11434/api/tags`
4.  Test Integration: `python backend/test_ollama.py`

### Log Functions
*   **Backend**: `d:\DevWorkspace\Roddle\backend\backend.log`
*   **Frontend**: `d:\DevWorkspace\Roddle\frontend\frontend.log`

### Known Issues
*   **Logout Logging**: There is a pending investigation into logout flow checkpoints (See `LOGGING_INVESTIGATION.md` reference in old notes).
*   **Difficulty Unlocks**: Previously reported issues with "Very Hard/Insane" unlocks; claimed fixed in recent Todo but verify if users report access issues.

---

## ðŸ§ª Testing Protocol
*referenced from `test_result.md`*

*   **Strategy**: Use `test_result.md` to track pass/fail state of features.
*   **Agent Role**: The `testing_agent` should be delegated tasks to verify specific fixes.
*   **Feedback Loop**: Update `test_result.md` before and after testing sessions.

